{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "\t<script>\n",
      "\t\tlocation.replace(location.href.replace(\"https://\",\"http://\"));\n",
      "\t</script>\n",
      "</head>\n",
      "<body>\n",
      "\t<noscript><meta http-equiv=\"refresh\" content=\"0;url=http://www.baidu.com/\"></noscript>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "#coding = utf-8\n",
    "import requests\n",
    "import re\n",
    "user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' \n",
    "\n",
    "headers = { 'User-Agent' : user_agent } \n",
    "#headers = {User-Agent: Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5 (.NET CLR 3.5.30729)}\n",
    "url = 'https://www.baidu.com/s?tn=news&rtt=4&bsst=1&cl=2&wd=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&medium=0'\n",
    "res = requests.get(url, headers=headers).text\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a45d7d9d43ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbase_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.baidu.com'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "base_url = 'https://www.baidu.com'\n",
    "driver.get(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_href = '<h3 class=\"c-title\">.*? <a href=\" \"'\n",
    "p_title = '<h3 class=\"c-title\">.*? >(.*? )</a >'\n",
    "p_info = '<p class=\"c-author\">(.*? )</p >'\n",
    "href = re.findall(p_href, res, re.S)\n",
    "title = re.findall(p_title, res, re.S)\n",
    "info = re.findall(p_info, res, re.S)\n",
    "\n",
    "source = []\n",
    "date = []\n",
    "for i in range(len(title)):\n",
    "    title[i] = title[i].strip()\n",
    "    title[i] = re.sub('<.*? >', '', title[i])\n",
    "    info[i] = re.sub('<.*? >', '', info[i])\n",
    "    source.append(info[i].split('&nbsp; &nbsp; ')[0])\n",
    "    date.append(info[i].split('&nbsp; &nbsp; ')[1])\n",
    "    source[i] = source[i].strip()\n",
    "    date[i] = date[i].strip()\n",
    "    print(str(i+1) + '.' + title[i] + '(' + date[i] + '-'+source[i] + ')')\n",
    "    print(href[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前正在爬取: https://www.baidu.com/s?tn=news&rtt=4&bsst=1&cl=2&wd=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&medium=0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findall'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b4c55f13151e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mBluenile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbluenileSpider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mBluenile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsePage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURL1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-b4c55f13151e>\u001b[0m in \u001b[0;36mparsePage\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"grid-body\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mlinks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"href\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mdetail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseDetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findall'"
     ]
    }
   ],
   "source": [
    "# -* coding: utf-8 *-\n",
    "#author: dtlin\n",
    "#data: 2020-05-23\n",
    "#descriptinon: 爬取游戏的详细信息，并将爬取的数据存储到CSV文\n",
    " \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "#from fake_useragent import UserAgent\n",
    "\n",
    "URL1=\"https://www.baidu.com/s?tn=news&rtt=4&bsst=1&cl=2&wd=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&medium=0\"\n",
    "#URL2=\"https://sh.lianjia.com/ditiefang/li143685064/pg{}mw1y4sf1l2l3a3a4p3p4/\"\n",
    "name=\"wangzherongyao\"\n",
    " \n",
    "class bluenileSpider(object):\n",
    " \n",
    "    def __init__(self):\n",
    "        headers={'User-Agent':'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'}\n",
    "        self.headers = headers\n",
    "        self.datas = list()\n",
    " \n",
    "\n",
    " \n",
    " \n",
    "    def parsePage(self, url):\n",
    "        #maxPage = self.getMaxPage(url)\n",
    "        \n",
    "        \n",
    "        print(\"当前正在爬取: {}\".format(url))\n",
    "        response = requests.get(url, headers = self.headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = soup.find(\"div\", class_ = \"grid-body\")\n",
    "        urls =  links.find_all(\"a\")[\"href\"]\n",
    "        for link in urls:    \n",
    "            detail = self.parseDetail(link)\n",
    "            self.datas.append(detail)\n",
    " \n",
    "        #  将所有爬取的二手房数据存储到csv文件中\n",
    "        data = pd.DataFrame(self.datas)\n",
    "        # columns字段：自定义列的顺序（DataFrame默认按列名的字典序排序）\n",
    "        columns = [\"价格 \", \"成色\", \"净度\", \"克拉重量\", \"荧光\", \"钻身高度\", \"桌面\", \"抛光度\", \"对称度\",\"腰棱\",\"底尖\",\"量度尺寸\",\"编号\"]\n",
    "        data.to_csv(\".\\bluenile.csv\", encoding='utf_8_sig', index=False, columns=columns)\n",
    " \n",
    " \n",
    "    def parseDetail(self, url):\n",
    "        response = requests.get(url, headers = self.headers)\n",
    "        detail = {}\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            detail[\"价格\"] = soup.find(\"span\", class_ = \"pricelabel\").text\n",
    "            detail[\"成色\"] = soup.find(\"span\", class_ = \"color-column-1\").text\n",
    "            detail[\"净度\"] = soup.find(\"span\", class_ = \"clarity-column-1\").text\n",
    "            detail[\"克拉重量\"] = soup.find(\"span\", class_ = \"carat-weight-column-1\").text\n",
    "            detail[\"荧光\"] = soup.find(\"span\", class_ = \"fluorescence-column-1\").text\n",
    "            detail[\"钻身高度\"] = soup.find(\"span\", class_ = \"depth--column-1\").text\n",
    "            detail[\"桌面\"] = soup.find(\"span\", class_ = \"table--column-1\").text\n",
    "            detail[\"抛光度\"] = soup.find(\"span\", class_ = \"polish-column-1\").text\n",
    "            detail[\"对称度\"] = soup.find(\"span\", class_ = \"symmetry-column-1\").text\n",
    "            detail[\"腰棱\"] = soup.find(\"span\", class_ = \"girdle-column-1\").text\n",
    "            detail[\"底尖\"] = soup.find(\"span\", class_ = \"culet-column-1\").text\n",
    "            detail[\"量度尺寸\"] = soup.find(\"span\", class_ = \"measurements-column-1\").text\n",
    "            detail[\"编号\"] = soup.find(\"span\", class_ = \"stock-number-column-1\").text\n",
    "            \n",
    "            return detail\n",
    "        else:\n",
    "            print(\"wrong\")\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    Bluenile = bluenileSpider()\n",
    "    Bluenile.parsePage(URL1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
